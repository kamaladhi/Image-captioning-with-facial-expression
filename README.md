# Image Captioning with Facial Expression  
*A combined pipeline for face‚Äëemotion detection and image captioning*

**Authors:** Jeevakamal K.‚ÄØR. ([@kamaladhi](https://github.com/kamaladhi)), Guhan K.‚ÄØB. ([@Guhanbala](https://github.com/Guhanbala)), Gowtham S.‚ÄØD. ([@GowthamDhanaraju](https://github.com/GowthamDhanaraju)), Shatha Varsha Sree T. ([@ShathaVarsha](https://github.com/ShathaVarsha))  
**Course:** Introduction to AI & Machine Learning  
**Supervisor:** Dr.‚ÄØKrithish‚ÄØGupta  
**Semester Project ‚Äì 2nd‚ÄØSemester**

---

## üìò Project Overview  
This repository presents a two‚Äëstage system that takes images containing human faces, detects the facial emotion (e.g., happy or sad), and then generates an image caption that incorporates the detected emotion for more expressive, enriched descriptions.

Key functionalities:  
- Real‚Äëtime (or static) detection of a human face‚Äôs emotion (happy vs. sad)  
- Image captioning model based on encoder‚Äëdecoder architecture (CNN encoder + Transformer decoder)  
- Integration of emotion information into the caption generation process using an NLP component (e.g., GPT‚Äë2)  
- Pipeline designed to combine vision (emotion + image features) and language (caption generation)  

---

## üîß Architecture & Modules  

### 1. Emotion Detection  
A Convolutional Neural Network (CNN) is trained (or fine‚Äëtuned) to classify each detected human face into one of two emotional states: **Happy** or **Sad**. Facial feature extraction ‚Üí emotion classification.

### 2. Image Captioning  
The image captioning module comprises:  
- **Encoder**: A CNN that extracts a feature vector from an input image.  
- **Decoder**: A Transformer or sequence model that takes the feature vector and generates a caption via a softmax‚Äìbased word‚Äëby‚Äëword language generation process.

### 3. Emotion + Caption Integration  
Once the emotion is detected and a base caption is generated, an NLP module (e.g., GPT‚Äë2 or another language model) takes both the base caption **and** the emotion label as input, and outputs a refined caption (max length ~30 words) that reflects both what‚Äôs visually present *and* the emotional state of the face.

---

## üìÅ Repository Contents  
| File / Folder              | Description                                                                 |
|----------------------------|-----------------------------------------------------------------------------|
| `Emotion_Recognition_CNN.ipynb` | Notebook for training & evaluating the face‚Äëemotion detection CNN.         |
| `Image_captioning_TRANSFORMERS.ipynb` | Notebook for training the encoder‚Äëdecoder captioning model.           |
| `NLP_Emotion+Captioning.ipynb`         | Notebook for the integration of emotion label and base caption into final refined caption. |
| `LICENSE`                        | MIT License (see below).                                                |
| `README.md`                      | (This file)                                                               |

---

## üõ†Ô∏è Setup & Usage  
1. Clone the repository:  
   ```bash
   git clone https://github.com/kamaladhi/Image-captioning-with-facial-expression.git
   cd Image-captioning-with-facial-expression
   ```  
2. Make sure you have the required environment (e.g., Python‚ÄØ3.x, Jupyter notebooks, PyTorch/TensorFlow, HuggingFace Transformers, OpenCV).  
3. **Emotion Detection** module: Open and execute `Emotion_Recognition_CNN.ipynb`. Train / load the CNN model and test on sample images.  
4. **Image Captioning** module: Open `Image_captioning_TRANSFORMERS.ipynb`. Train / load the captioning model, generate captions from images.  
5. **Integration Module**: Open `NLP_Emotion+Captioning.ipynb`. Pass the emotion label + base caption into the NLP model to produce final captions.  
6. (Optional) Extend or modify:  
   - Expand emotion classes (beyond happy/sad)  
   - Use other captioning architectures or datasets  
   - Deploy as a web service or integrate into a larger application  

---

## üéØ Potential Extensions  
- Increase the emotion classification granularity (e.g., angry, surprised, disgusted, neutral)  
- Use more sophisticated image captioning datasets (e.g., MS‚ÄØCOCO, Flickr30k) with transfer‚Äëlearning  
- Real‚Äëtime webcam feed: grab a frame, detect face + emotion, generate caption on the fly  
- User interface: simple web or mobile app interface to upload image ‚Üí show final caption  
- Evaluation metrics: For captioning (BLEU, METEOR, CIDEr), for emotion detection (accuracy, confusion matrix)

---

## ‚úÖ Limitations & Considerations  
- Current emotion detection module is limited to **two** classes (happy / sad) ‚Äî may misclassify other emotional states.  
- Captioning model is basic: depending on dataset size & compute resources, generated captions may be generic or inaccurate.  
- Integration of emotion into captioning is simple ‚Äî more nuanced emotional contexts (e.g., ‚Äúshe looks anxiously happy‚Äù) are not handled.  
- Ethical / privacy concerns: face detection/emotion recognition can raise issues in real‚Äëworld deployment (consent, bias, fairness).  
- Dataset biases and model overfitting: ensure proper validation and diverse data.

---

## üßë‚Äçüíª Contributors  
- [Jeevakamal‚ÄØK.‚ÄØR.](https://github.com/kamaladhi)  
- [Shatha‚ÄØVarsha‚ÄØSree‚ÄØT.](https://github.com/ShathaVarsha)  
- [Gowtham‚ÄØS.‚ÄØD.](https://github.com/GowthamDhanaraju)  
- [Guhan‚ÄØK.‚ÄØB.](https://github.com/Guhanbala)  

---

## üìÑ License  
This project is licensed under the MIT License ‚Äì see the [LICENSE](LICENSE) file for details.  
You are free to use, modify, and distribute this code, but please give appropriate credit.

---


